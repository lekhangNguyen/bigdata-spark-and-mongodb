{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZDs9jykvpvz"
      },
      "source": [
        "Group 12: Nguyen Le Khang, Nguyen Sieng\n",
        "\n",
        "Course: Big Data\n",
        "\n",
        "Final Project\n",
        "\n",
        "Topic: Client Attribute for Finance and Banking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_MdApuLpkcb"
      },
      "outputs": [],
      "source": [
        "# Install PySpark and MongoDB dependencies\n",
        "!pip install pyspark\n",
        "!pip install pymongo dnspython\n",
        "!pip install seaborn matplotlib\n",
        "!pip install avro\n",
        "!pip install streamlit\n",
        "!pip install -q streamlit pyngrok\n",
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "SQZZSP8AqM83",
        "outputId": "f9bf60a4-33f9-4ca2-e9e5-876386d945d3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3b64bfc1-dfbc-4cdf-85f5-20c21d41495f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-3b64bfc1-dfbc-4cdf-85f5-20c21d41495f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "upload = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-EUlPl2ApnE"
      },
      "outputs": [],
      "source": [
        "# PySpark and related libraries for Spark processing\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, when\n",
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# MongoDB Integration\n",
        "from pymongo import MongoClient\n",
        "\n",
        "# Data Handling\n",
        "import pandas as pd\n",
        "\n",
        "# Data Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        " #ML\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Streaming Context for Spark Streaming (Bonus)\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Streamlit for Dashboard (Bonus)\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VuCD193aq0bP"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('archive (1).zip')\n",
        "print(\"Shape:\", df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rfyDY6jIAuEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NErU7-dny8d"
      },
      "outputs": [],
      "source": [
        "## check for any missing values\n",
        "print(\"Missing values in each column:\")\n",
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c18_CU9Vosr_"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()\n",
        "df.isnull().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kHWhtVzo0Zb"
      },
      "outputs": [],
      "source": [
        "## check for duplicate values\n",
        "duplicates = df.duplicated()\n",
        "total_duplicate = duplicates.sum()\n",
        "print(f'Total duplicate record:\\n{total_duplicate}\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZnRtsZrpRa2"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJ6mUnYcqDZc"
      },
      "outputs": [],
      "source": [
        "month_dict = {\n",
        "    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5,\n",
        "    'jun': 6, 'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10,\n",
        "    'nov': 11, 'dec': 12\n",
        "}\n",
        "\n",
        "##replace month names with numbers\n",
        "df['month'] = df['month'].map(month_dict)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snbknmO8q6Fb"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2T5_cRJd3zC"
      },
      "outputs": [],
      "source": [
        "#Save to CSV\n",
        "df.to_csv(\"cleaned_bank_data.csv\", index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUgNQ4vmS1xh"
      },
      "outputs": [],
      "source": [
        "# Distribution of age\n",
        "sns.histplot(df['age'], kde=True)\n",
        "plt.title('Age Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSbYm7trq8U-"
      },
      "outputs": [],
      "source": [
        "import pymongo\n",
        "\n",
        "uri = \"mongodb+srv://lekhang23000:Lekhang2002@bigdatafinal.r0c6kdd.mongodb.net/?retryWrites=true&w=majority&appName=bigdataFinal\"\n",
        "client = MongoClient(uri)\n",
        "db = client[\"bank_data\"]\n",
        "collection = db[\"client\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ln0evqjhwiFR"
      },
      "outputs": [],
      "source": [
        "# Schema design: Store data with indexing\n",
        "data_dict = df.to_dict(\"records\")\n",
        "collection.insert_many(data_dict)\n",
        "\n",
        "# Indexing on frequently queried columns\n",
        "collection.create_index([(\"y\", pymongo.ASCENDING)])\n",
        "collection.create_index([(\"job\", pymongo.ASCENDING)])\n",
        "\n",
        "print(f\"Inserted {collection.count_documents({})} documents into MongoDB Atlas.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QUjUYwewtPL"
      },
      "outputs": [],
      "source": [
        "##first 5 documents\n",
        "for doc in collection.find().limit(5):\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4WnoL4Z5U1nY"
      },
      "outputs": [],
      "source": [
        "results = collection.find({\"age\": {\"$gt\": 30}}).limit(10)\n",
        "\n",
        "for result in results:\n",
        "    print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfdT41VTXh38"
      },
      "outputs": [],
      "source": [
        "#aggregation to calculate the subscription rate\n",
        "pipeline_subscription_rate = [\n",
        "    {\n",
        "        \"$facet\": {\n",
        "            \"total_clients\": [\n",
        "                {\"$count\": \"total\"}  #total number of clients\n",
        "            ],\n",
        "            \"subscribed_clients\": [\n",
        "                {\"$match\": {\"y\": \"yes\"}},  #filter for clients who subscribed\n",
        "                {\"$count\": \"subscribed\"}  #number of subscribed clients\n",
        "            ]\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "subscription_rate_result = collection.aggregate(pipeline_subscription_rate)\n",
        "\n",
        "#Ãªxtract\n",
        "subscription_rate = list(subscription_rate_result)\n",
        "if subscription_rate:\n",
        "    total_clients = subscription_rate[0]['total_clients'][0]['total']\n",
        "    subscribed_clients = subscription_rate[0]['subscribed_clients'][0]['subscribed']\n",
        "    subscription_rate_percentage = (subscribed_clients / total_clients) * 100\n",
        "    print(f\"Subscription Rate: {subscription_rate_percentage:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8TUY3Jkw7UQ"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Bank Marketing Data\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.5.0\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjXyepRNxA38"
      },
      "outputs": [],
      "source": [
        "spark_df = spark.read.csv(\"cleaned_bank_data.csv\", header=True, inferSchema=True)\n",
        "spark_df.printSchema()\n",
        "spark_df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xHS4x977yG7-"
      },
      "outputs": [],
      "source": [
        "##how many clients subscribed to the term deposit\n",
        "spark_df.groupBy(\"y\").count().show()\n",
        "##average age of clients by job type\n",
        "spark_df.groupBy(\"job\").avg(\"age\").orderBy(\"avg(age)\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw0MgfKVgh4F"
      },
      "source": [
        "Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbIKSOkHgU_C"
      },
      "outputs": [],
      "source": [
        "spark_df.selectExpr(\"avg(age)\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChnXsjOsgqQI"
      },
      "outputs": [],
      "source": [
        "spark_df.groupBy(\"marital\").count().orderBy(\"count\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErCJGrnKgt7B"
      },
      "outputs": [],
      "source": [
        "spark_df.groupBy(\"education\").count().orderBy(\"count\", ascending=False).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DO3G7GOLgxnr"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "total = spark_df.count()\n",
        "subscribed = spark_df.filter(col(\"y\") == \"yes\").count()\n",
        "print(f\"Subscription Rate: {(subscribed / total) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P53iKIUyg44X"
      },
      "source": [
        "2. Conceptual Feature Importance (prepare for model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gtPrKvyfg7cx"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "#categorical columns (except target)\n",
        "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome']\n",
        "\n",
        "#index these + target column 'y'\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_idx\") for col in cat_cols + ['y']]\n",
        "pipeline = Pipeline(stages=indexers)\n",
        "indexed_df = pipeline.fit(spark_df).transform(spark_df)\n",
        "\n",
        "#numeric columns + the \"_idx\" versions of categorical ones\n",
        "feature_cols = ['age', 'balance', 'day', 'month', 'duration', 'campaign', 'pdays', 'previous'] + [col + \"_idx\" for col in cat_cols]\n",
        "\n",
        "#assemble features into a single vector\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "final_df = assembler.transform(indexed_df).select(\"features\", \"y_idx\")\n",
        "\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"y_idx\")\n",
        "model = lr.fit(final_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s2iuUKxoFlM"
      },
      "outputs": [],
      "source": [
        "print(\"Feature Importances (Logistic Regression Coefficients):\")\n",
        "for name, coef in zip(feature_cols, model.coefficients):\n",
        "    print(f\"{name}: {coef:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqHA-rSKpLKX"
      },
      "outputs": [],
      "source": [
        "features = {\n",
        "    'age': 'Age',\n",
        "    'balance': 'Account Balance',\n",
        "    'day': 'Day of Month',\n",
        "    'month': 'Month',\n",
        "    'duration': 'Call Duration',\n",
        "    'campaign': 'Number of Contacts',\n",
        "    'pdays': 'Days Since Last Contact',\n",
        "    'previous': 'Previous Contacts',\n",
        "    'job_idx': 'Job (encoded)',\n",
        "    'marital_idx': 'Marital Status (encoded)',\n",
        "    'education_idx': 'Education (encoded)',\n",
        "    'default_idx': 'Default Status (encoded)',\n",
        "    'housing_idx': 'Housing Loan (encoded)',\n",
        "    'loan_idx': 'Personal Loan (encoded)',\n",
        "    'contact_idx': 'Contact Method (encoded)',\n",
        "    'poutcome_idx': 'Previous Outcome (encoded)'\n",
        "}\n",
        "coefficients = [\n",
        "    0.0006, 0.0000, -0.0033, -0.0062, 0.0040, -0.1035,\n",
        "    -0.0015, -0.0007, 0.0303, 0.1653, 0.0049,\n",
        "    -0.2696, 0.9844, -0.6139, -0.4274, 0.9106\n",
        "]\n",
        "\n",
        "#create DataFrame for easier plotting\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': list(features.values()),\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "#sort by absolute value\n",
        "coef_df['Abs'] = coef_df['Coefficient'].abs()\n",
        "coef_df = coef_df.sort_values(by='Abs', ascending=True)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "bars = plt.barh(coef_df['Feature'], coef_df['Coefficient'], color=coef_df['Coefficient'].apply(lambda x: 'green' if x > 0 else 'red'))\n",
        "plt.axvline(0, color='gray', linestyle='--', linewidth=1)\n",
        "plt.title(\"Logistic Regression Feature Importances (Smarter View)\", fontsize=14)\n",
        "plt.xlabel(\"Coefficient\", fontsize=12)\n",
        "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
        "\n",
        "#annotations\n",
        "for bar in bars:\n",
        "    width = bar.get_width()\n",
        "    plt.text(width + 0.02 if width > 0 else width - 0.02,\n",
        "             bar.get_y() + bar.get_height()/2,\n",
        "             f'{width:.3f}',\n",
        "             va='center',\n",
        "             ha='left' if width > 0 else 'right',\n",
        "             fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVTo7MEuAL3Q"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
        "# Define the schema for the streaming dataframe\n",
        "schema = StructType([\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"job\", StringType(), True),\n",
        "    StructField(\"marital\", StringType(), True),\n",
        "    StructField(\"education\", StringType(), True),\n",
        "    StructField(\"default\", StringType(), True),\n",
        "    StructField(\"housing\", StringType(), True),\n",
        "    StructField(\"loan\", StringType(), True),\n",
        "    StructField(\"contact\", StringType(), True),\n",
        "    StructField(\"month\", StringType(), True),\n",
        "    StructField(\"day_of_week\", StringType(), True),\n",
        "    StructField(\"duration\", IntegerType(), True),\n",
        "    StructField(\"campaign\", IntegerType(), True),\n",
        "    StructField(\"pdays\", IntegerType(), True),\n",
        "    StructField(\"previous\", IntegerType(), True),\n",
        "    StructField(\"poutcome\", StringType(), True),\n",
        "    StructField(\"y\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Batch Processing\n",
        "batch_df = spark.read.csv(\"cleaned_bank_data.csv\", header=True, inferSchema=True)\n",
        "batch_df.show(5)\n",
        "\n",
        "# Stream Processing with defined schema\n",
        "stream_df = spark.readStream.schema(schema).csv(\"cleaned_bank_data.csv\")\n",
        "\n",
        "# Check streaming data\n",
        "stream_df.isStreaming\n",
        "\n",
        "# Convert cleaned CSV to Parquet format (overwrite if the file already exists)\n",
        "parquet_path = \"cleaned_bank_data.parquet\"\n",
        "batch_df.write.mode(\"overwrite\").parquet(parquet_path)\n",
        "\n",
        "# Now we can read the Parquet file\n",
        "parquet_df = spark.read.parquet(parquet_path)\n",
        "parquet_df.show(5)\n",
        "\n",
        "# Measure execution time for a query using Parquet format\n",
        "from time import time\n",
        "\n",
        "start = time()\n",
        "parquet_df.groupBy(\"y\").count().show()\n",
        "print(f\"Query execution time with Parquet: {time() - start:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLeM6syfKOkq"
      },
      "outputs": [],
      "source": [
        "# Set page config as the first Streamlit command\n",
        "st.set_page_config(layout=\"wide\")\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"BankMarketingStreamlit\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Load data from Spark\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    # Load data from the CSV file and remove the 'Unnamed: 0' column\n",
        "    df_spark = spark.read.csv(\"cleaned_bank_data.csv\", header=True, inferSchema=True)\n",
        "    df_spark = df_spark.toPandas()  # Convert to pandas for Streamlit compatibility\n",
        "    if 'Unnamed: 0' in df_spark.columns:\n",
        "        df_spark.drop(columns=['Unnamed: 0'], inplace=True)\n",
        "    return df_spark\n",
        "\n",
        "# Load the data\n",
        "df = load_data()\n",
        "\n",
        "# Title of the app\n",
        "st.title(\"ðŸ“Š Bank Marketing Data Dashboard\")\n",
        "\n",
        "# Sidebar Filters\n",
        "st.sidebar.header(\"Filter Options\")\n",
        "job_filter = st.sidebar.multiselect(\"Select Job(s)\", df['job'].unique(), default=df['job'].unique())\n",
        "age_range = st.sidebar.slider(\"Select Age Range\", int(df['age'].min()), int(df['age'].max()), (20, 60))\n",
        "income_range = st.sidebar.slider(\"Select Income Range\", int(df['balance'].min()), int(df['balance'].max()), (0, 10000))\n",
        "marital_status = st.sidebar.multiselect(\"Select Marital Status\", df['marital'].unique(), default=df['marital'].unique())\n",
        "education_status = st.sidebar.multiselect(\"Select Education\", df['education'].unique(), default=df['education'].unique())\n",
        "housing_status = st.sidebar.multiselect(\"Select Housing\", df['housing'].unique(), default=df['housing'].unique())\n",
        "\n",
        "# Filtered DataFrame based on sidebar inputs\n",
        "filtered_df = df[\n",
        "    (df['job'].isin(job_filter)) &\n",
        "    (df['age'] >= age_range[0]) & (df['age'] <= age_range[1]) &\n",
        "    (df['balance'] >= income_range[0]) & (df['balance'] <= income_range[1]) &\n",
        "    (df['marital'].isin(marital_status)) &\n",
        "    (df['education'].isin(education_status)) &\n",
        "    (df['housing'].isin(housing_status))\n",
        "]\n",
        "\n",
        "# Key Metrics\n",
        "st.subheader(\"ðŸ“Œ Key Metrics\")\n",
        "col1, col2, col3, col4 = st.columns(4)\n",
        "col1.metric(\"Total Clients\", len(df))\n",
        "col2.metric(\"Subscribed\", df[df[\"y\"] == \"yes\"].shape[0])\n",
        "col3.metric(\"Subscription Rate (%)\", f\"{(df['y'].value_counts(normalize=True)['yes']*100):.2f}\")\n",
        "col4.metric(\"Avg Age\", f\"{df['age'].mean():.1f}\")\n",
        "\n",
        "# Data Preview\n",
        "st.subheader(\"ðŸ—ƒï¸ Data Preview\")\n",
        "st.dataframe(filtered_df.head(20), use_container_width=True)\n",
        "\n",
        "# Education Distribution\n",
        "st.subheader(\"ðŸŽ“ Education Distribution\")\n",
        "edu_counts = filtered_df[\"education\"].value_counts()\n",
        "st.bar_chart(edu_counts)\n",
        "\n",
        "# Age Histogram\n",
        "st.subheader(\"ðŸŽ‚ Age Distribution\")\n",
        "fig, ax = plt.subplots()\n",
        "sns.histplot(filtered_df[\"age\"], kde=True, color='skyblue', bins=20, ax=ax)\n",
        "ax.set_title(\"Age Distribution\")\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Subscription by Marital Status\n",
        "st.subheader(\"ðŸ’ Subscription by Marital Status\")\n",
        "sub_marital = pd.crosstab(filtered_df['marital'], filtered_df['y'], normalize='index') * 100\n",
        "st.bar_chart(sub_marital)\n",
        "\n",
        "# Avg Age by Job\n",
        "st.subheader(\"ðŸ‘” Average Age by Job\")\n",
        "avg_age_job = filtered_df.groupby('job')['age'].mean().sort_values()\n",
        "st.bar_chart(avg_age_job)\n",
        "\n",
        "# Subscription Rate by Job\n",
        "st.subheader(\"ðŸ’¼ Subscription Rate by Job\")\n",
        "sub_job = pd.crosstab(filtered_df['job'], filtered_df['y'], normalize='index') * 100\n",
        "st.bar_chart(sub_job)\n",
        "\n",
        "# Show raw data toggle\n",
        "if st.checkbox(\"Show full filtered data\"):\n",
        "    st.dataframe(filtered_df, use_container_width=True)\n",
        "\n",
        "# Additional Insights\n",
        "st.subheader(\"ðŸ“ˆ Additional Insights\")\n",
        "\n",
        "# Calculate and display correlation matrix\n",
        "st.write(\"#### Correlation Matrix\")\n",
        "numeric_cols = filtered_df.select_dtypes(include=['number']).columns  # only numeric columns\n",
        "corr_matrix = filtered_df[numeric_cols].corr()\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', ax=ax)\n",
        "st.pyplot(fig)\n",
        "\n",
        "# Interactive Predictions (ML Feature)\n",
        "if st.checkbox(\"Predict Subscription (ML)\"):\n",
        "\n",
        "    st.subheader(\"ðŸ”® Predict Subscription\")\n",
        "    st.write(\"This feature predicts subscription using a RandomForest model.\")\n",
        "\n",
        "    # Preprocessing: Label encode 'y', and handle categorical features\n",
        "    le = LabelEncoder()\n",
        "    df['y'] = le.fit_transform(df['y'])\n",
        "\n",
        "    # Prepare data\n",
        "    X = filtered_df[['age', 'balance', 'duration', 'campaign', 'job', 'marital', 'education', 'housing']]  # Features\n",
        "    X = pd.get_dummies(X, drop_first=True)  # One-hot encode categorical features\n",
        "    y = filtered_df['y']  # Target variable\n",
        "\n",
        "    # Train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    model = RandomForestClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    st.write(f\"Prediction Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    # Display model predictions for the first few test samples\n",
        "    st.write(\"Sample Predictions:\")\n",
        "    st.write(pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred}).head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rrB3vDEawoj"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "import subprocess\n",
        "#deactivate all active tunnels\n",
        "ngrok.kill()\n",
        "print(\"ngrok tunnels killed.\")\n",
        "\n",
        "#streamlit on port 8501\n",
        "process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port=8501\"])\n",
        "ngrok.set_auth_token(\"2vXGUWvKZRQO36cCpQeIMdvmCP4_6wq3k9wgERX4EgYrSR8RX\")\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is running at: {public_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tN4yhcUwqxkk"
      },
      "source": [
        "**Call Duration Is Critical**\n",
        "\n",
        "The duration variable has a strong positive coefficient (0.0040), meaning the longer the call, the higher the chance of a subscription.\n",
        "\n",
        "- Actionable Insight: Sales reps should aim for longer, engaging calls (quality matters!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbtFsrUxq0bO"
      },
      "source": [
        "**Loan Status Is a Strong Negative Indicator**\n",
        "\n",
        "loan_idx and default_idx have strong negative weights:\n",
        "\n",
        "Customers with personal loans or defaults are less likely to subscribe.\n",
        "\n",
        "- Use this to exclude low-potential leads, or tailor a different pitch for them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1VWM6hErLtx"
      },
      "source": [
        "**Housing Loan Holders Are More Likely to Subscribe**\n",
        "\n",
        "The highest positive coefficient is for housing_idx (0.9844).\n",
        "\n",
        "These customers might be more financially active or stable.\n",
        "\n",
        "- Good segment for upselling investment products."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjwBc46QruCR"
      },
      "source": [
        "**Marital Status & Job Matter**\n",
        "\n",
        "marital_idx has a decent positive weight (0.1653), job_idx has moderate influence.\n",
        "\n",
        "- Suggests that personal situations affect financial decision-making."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4d6BTp-rTHN"
      },
      "source": [
        "**Contact Method Has Strong Negative Impact**\n",
        "\n",
        "contact_idx is highly negative (âˆ’0.4274), suggesting certain contact methods reduce success rates (likely telephone vs cellular).\n",
        "\n",
        "- Optimize contact strategy by focusing on preferred or effective channels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZDpsnEGsMPu"
      },
      "source": [
        "**Month and Day Effects**\n",
        "\n",
        "Negative coefficients for month and day suggest:\n",
        "\n",
        "- Some months/days may be ineffective for campaigns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jk762y9fsfPe"
      },
      "source": [
        "**Historical Engagement Predicts Outcomes**\n",
        "poutcome_idx is highly positive (0.9106), meaning prior success leads to current success.\n",
        "\n",
        "- Re-target customers who previously responded well to marketing."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}